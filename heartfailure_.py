# -*- coding: utf-8 -*-
"""HeartFailure .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fXAH1OH7kLvIynrmy6vjy0p8C7WyfElm

#importing the libraries
"""

# 1. to handle the data
import pandas as pd
import numpy as np

# 2. To Viusalize the data
import matplotlib.pyplot as plt
import seaborn as sns

# 3. To preprocess the data
from sklearn.preprocessing import MinMaxScaler, LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.model_selection import StratifiedKFold,GridSearchCV

# 4. For Classification task.
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier

# 5. Metrics
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

"""#reading and uploding the dataset

"""

import kagglehub

# Download latest version
path = kagglehub.dataset_download("fedesoriano/heart-failure-prediction")

print("Path to dataset files:", path)

data= pd.read_csv("/kaggle/input/heart-failure-prediction/heart.csv")
df = data.copy()

"""#Data preproccesing"""

df.head()

df.shape

df.info()

df.isnull().sum()

plt.figure(figsize=(15,10))
sns.pairplot(df,hue="HeartDisease")
plt.title("Looking for Insites in Data")
plt.legend("HeartDisease")
plt.tight_layout()
plt.plot()

numeric_df = df.select_dtypes(include=["int64", "float64"])
corr = numeric_df.corr()
plt.figure(figsize=(15,15))
sns.heatmap(corr, cbar=True, square= True, fmt='.1f', annot=True, annot_kws={'size':15}, cmap='Greens')

"""#**Training our Machine Learning Model**

#Using Logistic Regression
"""

target = 'HeartDisease'
feature_cols = [col for col in df.columns if col != target]

categorical_cols = df.select_dtypes(include='object').columns
for col in categorical_cols:
    df[col] = LabelEncoder().fit_transform(df[col])

X = df[feature_cols]
y = df[target]

# Cross-validation
kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
acc_logreg = []
fold_num = 1

for train_idx, val_idx in kf.split(X, y):
    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

    #scaling the dataset
    scaler = MinMaxScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_val_scaled = scaler.transform(X_val)

    # Logistic Regression model
    model = LogisticRegression(max_iter=1000, random_state=42)
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_val_scaled)


    acc = accuracy_score(y_val, y_pred)
    acc_logreg.append(acc)

    print(f"Fold {fold_num} Accuracy: {acc:.4f}")
    print(classification_report(y_val, y_pred, digits=4))
    fold_num += 1


print(f"Mean Accuracy: {np.mean(acc_logreg):.4f}")

"""#Using SVM(Support Vector Machines)"""

acc_svm_rbf = []
for train_idx, val_idx in kf.split(X, y):
    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

    # scaling the dataset
    scaler = MinMaxScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_val_scaled = scaler.transform(X_val)

    # svm with rbf kernel
    model = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_val_scaled)

    acc = accuracy_score(y_val, y_pred)
    acc_svm_rbf.append(acc)

    print(f"Fold {fold_num} Accuracy: {acc:.4f}")
    print(classification_report(y_val, y_pred, digits=4))
    fold_num += 1

print(f"Mean Accuracy: {np.mean(acc_svm_rbf):.4f}")

"""#Using XGboost"""

kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
acc_xgb = []
fold_num = 1

for train_idx, val_idx in kf.split(X, y):
    # Split train and validation sets
    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

    # Scaling is optional for XGBoost but generally harmless
    scaler = MinMaxScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_val_scaled = scaler.transform(X_val)

    # Initialize XGBoost classifier
    model = xgb.XGBClassifier(
        n_estimators=100,
        learning_rate=0.1,
        max_depth=3,
        use_label_encoder=False,
        eval_metric='logloss',
        random_state=42
    )

    # Train the model
    model.fit(X_train_scaled, y_train)

    # Make predictions
    y_pred = model.predict(X_val_scaled)

    # Compute accuracy and classification report
    acc = accuracy_score(y_val, y_pred)
    acc_xgb.append(acc)

    print(f"Fold {fold_num} Accuracy: {acc:.4f}")
    print(classification_report(y_val, y_pred, digits=4))
    fold_num += 1

# Compute mean accuracy across all folds
print(f"Mean Accuracy: {np.mean(acc_xgb):.4f}")

"""#Using K-nearest Neighbors"""

# Features and target
X = df[feature_cols]
y = df[target]

# Feature scaling (essential for KNN)
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

# Define the KNN model
knn = KNeighborsClassifier()

# Define the parameter grid for k
param_grid = {'n_neighbors': list(range(1, 21))}  # Testing k from 1 to 20

# Use Stratified 5-Fold cross-validation
kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# GridSearchCV to find the best k
grid_search = GridSearchCV(
    estimator=knn,
    param_grid=param_grid,
    cv=kf,
    scoring='accuracy'
)

# Fit GridSearch
grid_search.fit(X_scaled, y)

# Best k and corresponding accuracy
print("Best k:", grid_search.best_params_['n_neighbors'])
print("Best cross-validated accuracy:", grid_search.best_score_)

# Detailed classification report using the best model
best_knn = grid_search.best_estimator_
y_pred = best_knn.predict(X_scaled)
print(classification_report(y, y_pred, digits=4))

acc_knn = []
fold_num = 1

for train_idx, val_idx in kf.split(X, y):
    # Split train and validation sets
    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

    # Feature scaling is essential for KNN
    scaler = MinMaxScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_val_scaled = scaler.transform(X_val)

    # Initialize KNN classifier
    model = KNeighborsClassifier(n_neighbors=5)  # You can tune k

    # Train the model
    model.fit(X_train_scaled, y_train)

    # Make predictions
    y_pred = model.predict(X_val_scaled)

    # Compute accuracy and classification report
    acc = accuracy_score(y_val, y_pred)
    acc_knn.append(acc)

    print(f"Fold {fold_num} Accuracy: {acc:.4f}")
    print(classification_report(y_val, y_pred, digits=4))
    fold_num += 1

# Compute mean accuracy across all folds
print(f"Mean Accuracy: {np.mean(acc_knn):.4f}")

"""#Using Random Forest Classifier"""

# 5-Fold Stratified Cross-Validation
kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
acc_rf = []
fold_num = 1

for train_idx, val_idx in kf.split(X, y):
    # Split train and validation sets
    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

    # Initialize Random Forest classifier
    model = RandomForestClassifier(
        n_estimators=100,
        max_depth=None,
        random_state=42
    )

    # Train the model
    model.fit(X_train, y_train)

    # Make predictions
    y_pred = model.predict(X_val)

    # Compute accuracy and classification report
    acc = accuracy_score(y_val, y_pred)
    acc_rf.append(acc)

    print(f"Fold {fold_num} Accuracy: {acc:.4f}")
    print(classification_report(y_val, y_pred, digits=4))
    fold_num += 1

# Compute mean accuracy across all folds
print(f"Mean Accuracy: {np.mean(acc_rf):.4f}")