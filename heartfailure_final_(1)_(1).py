# -*- coding: utf-8 -*-
"""HeartFailure_Final (1) (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1g-p8RKSaiWMDecLJ0j2bDeK5R6WOD6Jn

#**Data Preprocessing**

##**importing the libraries**
"""

# 1. to handle the data
import pandas as pd
import numpy as np

# 2. To Viusalize the data
import matplotlib.pyplot as plt
import seaborn as sns

# 3. To preprocess the data
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import MinMaxScaler, LabelEncoder,OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.model_selection import StratifiedKFold, train_test_split,GridSearchCV

# 4. For Classification task.
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
import xgboost as xgb

# 5. Metrics
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix
)

"""##**reading and uploding the dataset**

"""

import kagglehub

# Download latest version
path = kagglehub.dataset_download("fedesoriano/heart-failure-prediction")

print("Path to dataset files:", path)

data= pd.read_csv("/kaggle/input/heart-failure-prediction/heart.csv")
df = data.copy()

"""##**Dataset Information**"""

df.head()

df.shape

df.columns

df.info()

data.describe().T

yes = data[data['HeartDisease'] == 1].describe().T
no = data[data['HeartDisease'] == 0].describe().T
colors = ['#F93822','#FDD20E']

fig,ax = plt.subplots(nrows = 1,ncols = 2,figsize = (5,5))
plt.subplot(1,2,1)
sns.heatmap(yes[['mean']],annot = True,cmap = colors,linewidths = 0.4,linecolor = 'black',cbar = False,fmt = '.2f',)
plt.title('Heart Disease');

plt.subplot(1,2,2)
sns.heatmap(no[['mean']],annot = True,cmap = colors,linewidths = 0.4,linecolor = 'black',cbar = False,fmt = '.2f')
plt.title('No Heart Disease');

fig.tight_layout(pad = 2)
#Mean values of all the features for cases of heart diseases and non-heart diseases.

df.isnull().sum()

df['HeartDisease'].value_counts()

"""##**Data Analysis**"""

plt.figure(figsize=(15,10))
sns.pairplot(df,hue="HeartDisease")
plt.title("Looking for Insites in Data")
plt.legend("HeartDisease")
plt.tight_layout()
plt.plot()

numeric_df = df.select_dtypes(include=["int64", "float64"])
corr = numeric_df.corr()
plt.figure(figsize=(15,15))
sns.heatmap(corr, cbar=True, square= True, fmt='.1f', annot=True, annot_kws={'size':15}, cmap='Greens')

"""##**Splitting The Dataset Into Training Set and Test set**"""

X = df.drop("HeartDisease", axis=1)
y = df["HeartDisease"]
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

"""#**Training our Machine Learning Model**

##**Using Logistic Regression**
"""

categorical_cols = X_train.select_dtypes(include='object').columns
for col in categorical_cols:
    le = LabelEncoder()
    X_train[col] = le.fit_transform(X_train[col])
    X_test[col] = le.transform(X_test[col])

kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
acc_logreg = []
fold_num = 1

for train_idx, val_idx in kf.split(X_train, y_train):
    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]
    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]


    #scaling the dataset
    scaler = MinMaxScaler()
    X_tr_scaled = scaler.fit_transform(X_tr)
    X_val_scaled = scaler.transform(X_val)
    # Logistic Regression model
    model = LogisticRegression(max_iter=1000, random_state=42)
    model.fit(X_tr_scaled, y_tr)
    y_pred = model.predict(X_val_scaled)

    acc = accuracy_score(y_val, y_pred)
    acc_logreg.append(acc)

    print(f"Fold {fold_num} Accuracy: {acc:.4f}")
    print(classification_report(y_val, y_pred, digits=4))
    fold_num += 1


print(f"Mean Accuracy: {np.mean(acc_logreg):.4f}")

"""##**Using SVM(Support Vector Machines)**"""

categorical_cols = X_train.select_dtypes(include='object').columns
for col in categorical_cols:
    le = LabelEncoder()
    X_train[col] = le.fit_transform(X_train[col])
    X_test[col] = le.transform(X_test[col])
kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
acc_svm_rbf = []
fold_num = 1

for train_idx, val_idx in kf.split(X_train, y_train):
    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]
    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]

    scaler = MinMaxScaler()
    X_tr_scaled = scaler.fit_transform(X_tr)
    X_val_scaled = scaler.transform(X_val)


    model = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)
    model.fit(X_tr_scaled, y_tr)
    y_pred = model.predict(X_val_scaled)

    # ÿØŸÇÿ™ Ÿà ⁄Øÿ≤ÿßÿ±ÿ¥
    acc = accuracy_score(y_val, y_pred)
    acc_svm_rbf.append(acc)
    print(f"Fold {fold_num} Accuracy: {acc:.4f}")
    print(classification_report(y_val, y_pred, digits=4))
    fold_num += 1

print(f"Mean Cross-Validation Accuracy (SVM RBF): {np.mean(acc_svm_rbf):.4f}")

"""##**Using XGboost**"""

# Make safe copies to avoid SettingWithCopyWarning
Xtr_all = X_train.copy()
Xte_all = X_test.copy()

# Encode categorical columns: fit on train, transform on test
categorical_cols = Xtr_all.select_dtypes(include='object').columns
for col in categorical_cols:
    le = LabelEncoder()
    Xtr_all[col] = le.fit_transform(Xtr_all[col].astype(str))
    Xte_all[col] = le.transform(Xte_all[col].astype(str))

# Cross-validation on training data
kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
acc_xgb = []
fold_num = 1

for train_idx, val_idx in kf.split(Xtr_all, y_train):
    X_tr, X_val = Xtr_all.iloc[train_idx], Xtr_all.iloc[val_idx]
    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]

    # Fit scaler only on training fold (to avoid data leakage)
    scaler = MinMaxScaler()
    X_tr_scaled = scaler.fit_transform(X_tr)
    X_val_scaled = scaler.transform(X_val)

    # Initialize XGBoost classifier
    model = xgb.XGBClassifier(
        n_estimators=100,
        learning_rate=0.1,
        max_depth=3,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        eval_metric='logloss'  # avoids warning in recent versions
        # if you get an error about 'use_label_encoder', just remove it
    )

    # Train the model
    model.fit(X_tr_scaled, y_tr)

    # Predict on validation fold
    y_pred = model.predict(X_val_scaled)

    # Evaluate
    acc = accuracy_score(y_val, y_pred)
    acc_xgb.append(acc)

    print(f"Fold {fold_num} Accuracy: {acc:.4f}")
    print(classification_report(y_val, y_pred, digits=4))
    fold_num += 1

print(f"Mean Cross-Validation Accuracy (XGBoost): {np.mean(acc_xgb):.4f}")

"""##**Using K-nearest Neighbors**"""

#using gridsearchcv to find the hyperparameter k

# Feature scaling (essential for KNN)
scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train)

# Define the KNN model
knn = KNeighborsClassifier()

# Define the parameter grid for k
param_grid = {'n_neighbors': list(range(1, 21))}  # Testing k from 1 to 20

# Use Stratified 5-Fold cross-validation
kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# GridSearchCV to find the best k
grid_search = GridSearchCV(
    estimator=knn,
    param_grid=param_grid,
    cv=kf,
    scoring='accuracy'
)

# Fit GridSearch
grid_search.fit(X_train_scaled, y_train)

# Best k and corresponding accuracy
print("Best k:", grid_search.best_params_['n_neighbors'])
print("Best cross-validated accuracy:", grid_search.best_score_)

# Detailed classification report using the best model
best_knn = grid_search.best_estimator_
y_pred = best_knn.predict(X_train_scaled)
print(classification_report(y_train, y_pred, digits=4))

categorical_cols = X_train.select_dtypes(include='object').columns
for col in categorical_cols:
    le = LabelEncoder()
    X_train[col] = le.fit_transform(X_train[col])
    X_test[col] = le.transform(X_test[col])
kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
acc_knn = []
fold_num = 1

for train_idx, val_idx in kf.split(Xtr_all, y_train):
    X_tr, X_val = Xtr_all.iloc[train_idx], Xtr_all.iloc[val_idx]
    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]

    # Fit scaler only on training fold (to avoid data leakage)
    scaler = MinMaxScaler()
    X_tr_scaled = scaler.fit_transform(X_tr)
    X_val_scaled = scaler.transform(X_val)
    # Initialize KNN classifier
    model = KNeighborsClassifier(n_neighbors=13)

    # Train the model
    model.fit(X_tr_scaled, y_tr)

    # Make predictions
    y_pred = model.predict(X_val_scaled)

    # Compute accuracy and classification report
    acc = accuracy_score(y_val, y_pred)
    acc_knn.append(acc)

    print(f"Fold {fold_num} Accuracy: {acc:.4f}")
    print(classification_report(y_val, y_pred, digits=4))
    fold_num += 1

# Compute mean accuracy across all folds
print(f"Mean Accuracy: {np.mean(acc_knn):.4f}")

"""##**Using Random Forest Classifier**"""

# 5-Fold Stratified Cross-Validation
kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
acc_rf = []
fold_num = 1

for train_idx, val_idx in kf.split(Xtr_all, y_train):
    X_tr, X_val = Xtr_all.iloc[train_idx], Xtr_all.iloc[val_idx]
    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]
    # Initialize Random Forest classifier
    model = RandomForestClassifier(
        n_estimators=100,
        max_depth=None,
        random_state=42
    )

    # Train the model
    model.fit(X_tr, y_tr)

    # Make predictions
    y_pred = model.predict(X_val)

    # Compute accuracy and classification report
    acc = accuracy_score(y_val, y_pred)
    acc_rf.append(acc)

    print(f"Fold {fold_num} Accuracy: {acc:.4f}")
    print(classification_report(y_val, y_pred, digits=4))
    fold_num += 1

# Compute mean accuracy across all folds
print(f"Mean Accuracy: {np.mean(acc_rf):.4f}")

"""
# üìä Model Evaluation & Comparison

This section trains multiple **classification models** on the Heart Failure dataset and compares them using **Accuracy, Precision, Recall, F1-score**, and **ROC-AUC**.  
It also selects the **best model** based on F1-score and explains **why** it is a good fit for this dataset.
"""

from sklearn.pipeline import Pipeline

# Separate X, y
X = df.drop("HeartDisease", axis=1)
y = df["HeartDisease"]
# identify categorical and numeric columns by dtype + known names in Kaggle dataset
known_cats = ["Sex","ChestPainType","RestingECG","ExerciseAngina","ST_Slope"]
categorical_cols = [c for c in X.columns if (str(X[c].dtype) in ("object","category")) or (c in known_cats)]
numeric_cols = [c for c in X.columns if c not in categorical_cols]

# Preprocessors
categorical_transformer = OneHotEncoder(handle_unknown="ignore", sparse_output=False)

# Pipelines per model
# For linear/SVM/KNN we benefit from scaling numeric features.
preprocess_scaled = ColumnTransformer(
    transformers=[
        ("cat", categorical_transformer, categorical_cols),
        ("num",  StandardScaler(), numeric_cols)
    ],
    remainder="drop",
)

# For tree-based/GaussianNB: trees don't need scaling; NB can work with scaled or unscaled; we still OHE categoricals.
preprocess_no_scale = ColumnTransformer(
    transformers=[
        ("cat", categorical_transformer, categorical_cols),
        ("num", "passthrough", numeric_cols)
    ],
    remainder="drop",
)
models = {
    "LogisticRegression": Pipeline([
        ("prep", preprocess_scaled),
        ("clf", LogisticRegression(max_iter=2000, n_jobs=None, class_weight=None))
    ]),
    "KNN": Pipeline([
        ("prep", preprocess_scaled),
        ("clf", KNeighborsClassifier(n_neighbors=5))
    ]),
    "SVM (RBF)": Pipeline([
        ("prep", preprocess_scaled),
        ("clf", SVC(kernel="rbf", probability=True))
    ]),
    "RandomForest": Pipeline([
        ("prep", preprocess_no_scale),
        ("clf", RandomForestClassifier(n_estimators=300, random_state=42))
    ])
}
results = []

for name, pipe in models.items():
    pipe.fit(X_train, y_train)
    y_pred = pipe.predict(X_test)
     # Probabilities for ROC-AUC if available
    if hasattr(pipe.named_steps[list(pipe.named_steps.keys())[-1]], "predict_proba"):
        y_proba = pipe.predict_proba(X_test)[:, 1]
        roc = roc_auc_score(y_test, y_proba)
    else:
        # fallback using decision function or set to NaN
        if hasattr(pipe.named_steps[list(pipe.named_steps.keys())[-1]], "decision_function"):
            from sklearn.metrics import roc_auc_score
            scores = pipe.decision_function(X_test)
            # Convert to probability-like via ranking; or compute ROC on scores directly
            roc = roc_auc_score(y_test, scores)
        else:
            roc = np.nan

    acc = accuracy_score(y_test, y_pred)
    prec = precision_score(y_test, y_pred, zero_division=0)
    rec = recall_score(y_test, y_pred, zero_division=0)
    f1 = f1_score(y_test, y_pred, zero_division=0)

    results.append({
        "Model": name,
        "Accuracy": acc,
        "Precision": prec,
        "Recall": rec,
        "F1": f1,
        "ROC_AUC": roc
    })

results_df = pd.DataFrame(results).sort_values(by=["F1","Accuracy"], ascending=False).reset_index(drop=True)
display(results_df)

# Save results
results_df.to_csv("model_comparison_results.csv", index=False)

# Plot Accuracy Bar Chart
plt.figure(figsize=(8,5))
plt.bar(results_df["Model"], results_df["Accuracy"])
plt.title("Model Accuracy Comparison")
plt.ylabel("Accuracy")
plt.xticks(rotation=30, ha="right")
plt.tight_layout()
plt.show()

# Select best by F1
best_row = results_df.iloc[0]
best_model_name = str(best_row["Model"])
print(f"\n Best model by F1-score: {best_model_name}")
print(best_row)

# Show classification report and confusion matrix for the best model
best_pipe = models[best_model_name]
y_pred_best = best_pipe.predict(X_test)
print("\nClassification Report (Best Model):\n")
print(classification_report(y_test, y_pred_best, digits=4))

cm = confusion_matrix(y_test, y_pred_best)
print("Confusion Matrix:")
print(cm)

"""# **Model Comparison & Results**
In this section, we will compare all trained models based on multiple metrics (Accuracy, Precision, Recall, F1 Score, and ROC-AUC). This helps us select the best-performing algorithm for the Heart Failure Prediction dataset.

"""

# Model Comparison Section

# Dictionary to store model results
results = []

# Loop through all trained models and evaluate them
for name, model in models.items():  # models should be a dictionary like {"Logistic Regression": log_model, ...}
    y_pred = model.predict(X_test)

    # Calculate metrics
    acc = accuracy_score(y_test, y_pred)
    prec = precision_score(y_test, y_pred)
    rec = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])

    # Save results
    results.append({
        "Model": name,
        "Accuracy": acc,
        "Precision": prec,
        "Recall": rec,
        "F1 Score": f1,
        "ROC-AUC": auc
    })

# Convert results into a DataFrame
results_df = pd.DataFrame(results)

# Sort models by F1 Score (or any metric you want)
results_df = results_df.sort_values(by="F1 Score", ascending=False)

# Show results
print("Model Comparison Results:")
display(results_df)

# Save results to CSV (optional, for GitHub repository)
results_df.to_csv("model_comparison_results.csv", index=False)

# Plot a bar chart for better visualization
plt.figure(figsize=(10,6))
results_df.set_index("Model")[["Accuracy","Precision","Recall","F1 Score","ROC-AUC"]].plot(kind="bar")
plt.title("Model Performance Comparison")
plt.ylabel("Score")
plt.ylim(0,1)
plt.legend(loc="lower right")
plt.grid(axis="y")
plt.show()

"""#**Conclusion**

# ü©∫ Heart Failure Prediction ‚Äì Conclusion  

In this project, we explored different **machine learning algorithms** for predicting heart failure risk, including:  

- üîπ Logistic Regression  
- üîπ K-Nearest Neighbors (KNN)  
- üîπ Support Vector Machine (SVM)  
- üîπ Decision Tree  
- üîπ Random Forest  

---

## üìä Model Comparison  
Each model was evaluated using **Accuracy, Precision, Recall, F1-score, and ROC-AUC**.  
Among all, the **üå≤ Random Forest Classifier** achieved the **best overall performance**, striking a strong balance between **recall** (catching most positive cases) and **precision** (avoiding false alarms).  

---

## ‚úÖ Why Random Forest Works Best
- Handles both **linear & non-linear** patterns  
- Reduces **overfitting** compared to a single decision tree  
- Provides **feature importance** ‚Üí helpful for medical interpretation  

---

## üèÅ Final Takeaway  
While **Random Forest** was the top-performing model, **Logistic Regression** also showed good results and remains valuable when **simplicity & interpretability** are essential (e.g., in clinical decision-making).  

‚ö°Ô∏è **Bottom line:** For this dataset, ensemble models like Random Forest are most effective, but real-world healthcare applications must balance **accuracy, interpretability, and computational cost**.
"""